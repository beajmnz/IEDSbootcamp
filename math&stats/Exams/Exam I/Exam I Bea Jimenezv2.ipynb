{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTNT78uWIBEf"
   },
   "source": [
    "# <font color = \"slateblue\"> EXAM I </font>\n",
    "#### <font color = \"slateblue\"> Math and Stats</font>\n",
    "#### <font color = \"slateblue\"> Date: 11th, Feb 2021 </font>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJCyXQtnIbHE"
   },
   "source": [
    "## <font color = \"limegreen\">Student Data</font>\n",
    "\n",
    "Fill your **NAME** only:\n",
    "\n",
    "#### Student Name: ..........Bea....................\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aH2sF02mIbaW"
   },
   "source": [
    "## <font color = \"limegreen\">Instructions</font>\n",
    "\n",
    "Read carefully these instructions and follow them during your quiz and in your submission.\n",
    "\n",
    " * The exam lasts **1 week**\n",
    " * Read carefully the questions and do not answer before knowing what is asked\n",
    " * Full marks require **full explanations**. Just answering the question is not enough, for example, if one answer is that the type of data is *panel data*, just saying that will not grant you more than the 25% of the available points.\n",
    " * The **answers** must be written right below the questions made in this notebook. Use Code and Text cells as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CethReyNCKjw"
   },
   "source": [
    "---\n",
    "\n",
    "## <font color = \"limegreen\"> Packages </font>\n",
    "\n",
    "In the next cell code add **ALL** the modules you will use in your exam: `numpy`, `pandas`,... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37oQMoOeZIYj"
   },
   "outputs": [],
   "source": [
    "# this extension helps improve code readability\n",
    "%load_ext nb_black\n",
    "###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-BBasPpVw0u"
   },
   "source": [
    "In case you work with colab notebooks, use the following cell to connect to your drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-MwZPZ7ZH8A"
   },
   "outputs": [],
   "source": [
    "# not necessary because we won't be using collab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25-MIZMOG_S9"
   },
   "source": [
    "---\n",
    "\n",
    "## <font color = \"limegreen\">The Data </font>\n",
    "\n",
    "**A study about the scientific reasearch status around the world contains data from countries in Europe and in America in year 2016. It contains the following variables:**\n",
    "\n",
    "<br>\n",
    "\n",
    "| Variable    |           Description           |\n",
    "|-------------|---------------------------------|\n",
    "|  country    | country name |\n",
    "|  continent  | continent to which the country belongs |\n",
    "|  RDExpen    | Research and development expenditure (% of GDP)|\n",
    "|  STJpapers  | Scientific and technical journal articles |\n",
    "| researchers | Researchers in R&D (per million people) | \n",
    "<br>\n",
    "\n",
    "**in order to load it, use the next *code cell*, taking into account that it is an Excel file, not a .csv. Add in this same code cell a instruction that lets you see the 5 first rows of the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qj61LsOBZGQO"
   },
   "outputs": [],
   "source": [
    "# read the file and show the first rows to check everything is ok\n",
    "science_data = pd.read_excel(\"science_indicators.xlsx\")\n",
    "science_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epF8b3oDO79L"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4rd2VM8QDJ_"
   },
   "source": [
    "## 1.- <font color = \"Red\"> The Data Set </font>\n",
    "\n",
    "**In this first part, let's briefly describe the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eedtladhPAUo"
   },
   "source": [
    "### 1.1.- <font color = \"Blue\">Data (1 Point)</font>\n",
    "\n",
    "**From the statistics point of view, what type of data can you find in the study?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXPlskqmPQpO"
   },
   "source": [
    "The data supplied is *cross-sectional* because we have **collected** scientific-related **information** (the randomness applies in the sense that there might be data not included, even for a country for which we do have scientific information) **from** a bunch of **different individuals** (countries) **during the same period of time**, the year 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pM8W6oyzPQ8G"
   },
   "source": [
    "### 1.2.- <font color = \"Blue\">Variables (1 Point)</font>\n",
    "\n",
    "**Describe the type of variables you can find in the study both, from the point of view of their nature and from the point of view of their role (again from the statistics point of view)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bD807GJdPgvL"
   },
   "source": [
    "There are 5 variables in the data set.<br>\n",
    "<br>\n",
    "From the <b>nature</b> point of view:<br>\n",
    "<ul><li> <i>Country Name</i> - categorical, nominal</li>\n",
    "    <li> <i>Continent</i> - categorical, nominal</li>\n",
    "    <li> <i>RDExpen</i> - numerical, continuous (as it measures the ratio between the expenditure and the GDP)</li>\n",
    "    <li> <i>STJpapers</i> - numerical, apparently continuous (it's a little extrange that we talk about papers in decimals)</li>\n",
    "    <li> <i>researchers</i> - numerical, continuous (as it is presented as a ratio between population and scientists)</li></ul>\n",
    "<br>\n",
    "Regarding their <b>roles</b>:<br>\n",
    "<ul><li> <i>Country Name</i> - confounding. Science expense would be enough to analyze our information, but we cannot disregard which country we are talking about (same size countries may have different interests in science and, moreover, the expense variable is presented as a percentage of each country's GDP).</li>\n",
    "    <li> <i>Continent</i> - confounding but dependent on <i>Country Name</i> (every country belongs to a continent and it cannot be changed (at least not that easily))</li>\n",
    "    <li> <i>RDExpen</i> - explanatory but dependent on <i>Country Name</i> (every country has a science budget as a percentage of their GDP that may differ depending on their interest in science)</li>\n",
    "    <li> <i>STJpapers</i> - response, dependent on <i>RDExpen</i> and <i>researchers</i> (the more money a country invests in science, the more researchers can be hired and therefore, more scientific papers produced)</li>\n",
    "    <li> <i>researchers</i> - can be both explanatory and response, depending on our point of view. If we take it as explanatory and therefore independent, we might argue that the more researchers a country hires, the more papers they will produce. But we can also consider it a response variable, dependent on the amount of R+D expense: the more money available the more researchers will be hired.</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JweYf5DGhpKE"
   },
   "source": [
    "### 1.3.- <font color = \"Blue\">Population and Sample (1 Point)</font>\n",
    "\n",
    "**Explain which is the population of interest and which are the statistical units**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZEh8HtChpX3"
   },
   "source": [
    "The population is the whole scientific research in Europe and America, out of which we got information from some European and American countries (the statistical units)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DpLLXq6PhVE"
   },
   "source": [
    "### 1.4.- <font color = \"Blue\">Sample Size (1 Point)</font>\n",
    "\n",
    "**Determine the sample size using Python. Then determine how many countries belong to Europe and how many to America in this sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJh6LJ06L_Qr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The sample size is\",\n",
    "    science_data.shape[0],\n",
    "    \"countries for which we have science-related information.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continents = (\n",
    "    science_data.loc[:, [\"Country Name\", \"Continent\"]].groupby(by=\"Continent\").count()\n",
    ")\n",
    "print(\n",
    "    \"There are\",\n",
    "    continents.at[\"america\", \"Country Name\"],\n",
    "    \"American countries and\",\n",
    "    continents.at[\"europe\", \"Country Name\"],\n",
    "    \"European countries in our sample data.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adglUSz79QsY"
   },
   "source": [
    "### 1.5.- <font color = \"Blue\">Sizes (1 Point)</font>\n",
    "\n",
    "**If you split the `RDExpen` and `STJpapers` variables in three levels denoted by `low`, `mid` and `high` (using the own range of the variables), determine the number of countries in our sample that are in each of the levels generated by this division**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first check if we have RDExpen data for all the countries\n",
    "science_data[\"RDExpen\"].count() == science_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we check if we have STJpapers data for all the countries\n",
    "science_data[\"STJpapers\"].count() == science_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we do for STJpapers but not for RDExpen, so we must take that into account\n",
    "# by adding empty values also in our categorical variables\n",
    "\n",
    "# we are doing the same process twice, so we are going to create a for loop\n",
    "vars2cat = [\"RDExpen\", \"STJpapers\"]\n",
    "\n",
    "for var in vars2cat:\n",
    "    # first define the limits for the low, mid and high values. we choose\n",
    "    # the Q1 and Q3 quartiles.\n",
    "    low_limit = science_data[var].quantile(0.25)\n",
    "    mid_limit = science_data[var].quantile(0.75)\n",
    "\n",
    "    # then we add the new categorical variable\n",
    "    science_data[var + \"_cat\"] = np.where(\n",
    "        np.isnan(science_data[var]),\n",
    "        None,\n",
    "        (\n",
    "            np.where(\n",
    "                science_data[var] <= low_limit,\n",
    "                \"low\",\n",
    "                (np.where(science_data[var] <= mid_limit, \"mid\", \"high\")),\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # display(science_data[var + \"_cat\"].value_counts())\n",
    "    print(\n",
    "        \"There are\",\n",
    "        science_data[var + \"_cat\"].value_counts()[\"low\"],\n",
    "        \"countries in the low level,\",\n",
    "        science_data[var + \"_cat\"].value_counts()[\"mid\"],\n",
    "        \"countries in the mid level and\",\n",
    "        science_data[var + \"_cat\"].value_counts()[\"high\"],\n",
    "        \"countries in the high level of\",\n",
    "        var,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkdJTcUJOZY4"
   },
   "source": [
    "## 2.- <font color = \"Red\"> EDA </font>\n",
    "\n",
    "**The next questions are for the variables in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py2zP2styEBo"
   },
   "source": [
    "### 2.1.- <font color = \"Blue\"> Graphical Analysis (2 Points) </font> \n",
    "\n",
    "**Plot side by side its histogram and the boxplot. Then answer the following questions:**\n",
    "\n",
    " * **Is the distribution symmetric? Which value of skewness would you expect?**\n",
    " * **Do you detect any outliers? Which value of the excess kurtosis would you expect?**\n",
    " * **Which central tendency and variability measures would you use to describe the distribution?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wW86R7myWFhU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we are going to repeat the same plots for the three variables, so we better use a for loop\n",
    "dict_plots = {\n",
    "    \"RDExpen\": {\"plot_order\": 0, \"main_color\": \"green\", \"sec_color\": \"lightgreen\"},\n",
    "    \"STJpapers\": {\"plot_order\": 1, \"main_color\": \"red\", \"sec_color\": \"lightcoral\"},\n",
    "    \"researchers\": {\"plot_order\": 2, \"main_color\": \"blue\", \"sec_color\": \"lightblue\"},\n",
    "}\n",
    "\n",
    "# first we configure the main plot\n",
    "plt.figure(figsize=(16, 18))\n",
    "\n",
    "# then we loop to create two plots for each variable\n",
    "for dict_plot in dict_plots:\n",
    "\n",
    "    # then we define the first subplot with the histogram, and if there were empty values, we discard them\n",
    "    plt.subplot2grid((3, 2), (dict_plots[dict_plot][\"plot_order\"], 0))\n",
    "    plt.hist(\n",
    "        science_data[dict_plot].dropna(),\n",
    "        ec=dict_plots[dict_plot][\"main_color\"],\n",
    "        color=dict_plots[dict_plot][\"sec_color\"],\n",
    "        bins=\"rice\",\n",
    "    )\n",
    "    plt.title(dict_plot + \" Histogram\", fontsize=15)\n",
    "    plt.xlabel(\"Values\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "\n",
    "    # to create the boxplot we must discard empty values again if any\n",
    "    plt.subplot2grid((3, 2), (dict_plots[dict_plot][\"plot_order\"], 1))\n",
    "    plt.boxplot(\n",
    "        science_data[dict_plot].dropna(),\n",
    "        widths=0.6,\n",
    "        patch_artist=True,\n",
    "        showmeans=True,\n",
    "        whis=1.5,\n",
    "        labels=[dict_plot],\n",
    "        boxprops=dict(facecolor=dict_plots[dict_plot][\"sec_color\"]),\n",
    "        flierprops=dict(\n",
    "            marker=\"o\", markerfacecolor=dict_plots[dict_plot][\"main_color\"]\n",
    "        ),\n",
    "    )\n",
    "    plt.title(dict_plot + \" Boxplot\", fontsize=15)\n",
    "    plt.ylabel(\"Values\", fontsize=12)\n",
    "\n",
    "# last, we print everything on screen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we will describe the <i>RDExpen</i> distribution.\n",
    "\n",
    "It is not symmetric, but right-skewed, which means many scientific budgets in Europe and America are tight but there are also a significant number of countries with high investments in science. We'd expect a skewness value greater than 0. Let's calculate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ske = science_data.RDExpen.skew()\n",
    "ske"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no outliers in the plot, and the weight of the tails seems pretty small, specially on one side, so we would say the distribution is playkurtic and should expect a negative kurtosis value. Let's check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kur = science_data.RDExpen.kurt()\n",
    "kur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is not a symmetric distribution, we discard the mean as a proper central tendency measure, and instead use the median to better describe it. From the boxplot we can tell it should be aorund 1.4. Let's calculate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med = science_data.RDExpen.median()\n",
    "med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following our comment just above, we use IQR as the variability measure to describe our distribution. From the boxplot we can tell it should be around 1.6 (a little above 2.0 minus a little below 0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQR = science_data[\"RDExpen\"].quantile(0.75) - science_data[\"RDExpen\"].quantile(0.25)\n",
    "IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYRFTzewSxkn"
   },
   "source": [
    "### 2.2.- <font color = \"Blue\">Quantitative Analysis (2 Points)</font>\n",
    "\n",
    "**Make a summary with ALL the numerical quantities needed to describe the distribution. Then interpret them with respect to your arguments in 2.1. Did your expectations match with the numerical results? Explain.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VzScuLaPjjN"
   },
   "outputs": [],
   "source": [
    "print(\"{:^30}\".format(\"CENTRAL TENDENCY\"))\n",
    "print(\"-\" * 30)\n",
    "print(\"{:<25}\".format(\"Mean:\"), np.round(science_data.RDExpen.mean(), 2))\n",
    "print(\"{:<25}\".format(\"Median:\"), np.round(med, 2))\n",
    "print(\"{:<2}\".format(\"Mode:\"), [i for i in science_data.RDExpen.mode()])\n",
    "print(\"\\n{:^30}\".format(\"VARIABILITY\"))\n",
    "print(\"-\" * 30)\n",
    "print(\"{:<25}\".format(\"St. Dev.:\"), np.round(science_data.RDExpen.std(), 2))\n",
    "print(\"{:<25}\".format(\"Q1:\"), np.round(science_data.RDExpen.quantile(0.25), 2))\n",
    "print(\"{:<25}\".format(\"Q3:\"), np.round(science_data.RDExpen.quantile(0.75), 2))\n",
    "print(\"{:<25}\".format(\"IQR:\"), np.round(IQR, 2))\n",
    "print(\"{:<22}\".format(\"Minimum:\"), science_data.RDExpen.min())\n",
    "print(\"{:<22}\".format(\"Maximum:\"), science_data.RDExpen.max())\n",
    "print(\"\\n{:^30}\".format(\"SHAPE\"))\n",
    "print(\"-\" * 30)\n",
    "print(\"{:<25}\".format(\"Skewness:\"), np.round(ske, 2))\n",
    "print(\"{:<24}\".format(\"Kurtosis:\"), np.round(kur, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median, IRQ, skewness and kurtosis values match our expectations, as we showed in question 2.2. It is outstanding that there is no single mode value, but if we take into account this is continous numerical variable it is not surprising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVUYh1gWy_Nq"
   },
   "source": [
    "### 2.3.- <font color = \"Blue\"> Dependency (2 Points)</font>\n",
    "\n",
    "**Analyze from the point of view of association, correlation and relationship the dependency of the Scientific and technical journal articles, variable `STJpapers` (independent) with the Research and development expenditure, variable `RDExpen` (response). Make the proper graph for this analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we stated in question 1.2, variable \"STJpapers\" is for us the response variable and \"RDExpen\" is independent,\n",
    "# therefore we will make the analysis following that assumption (probably wrong, but at least we will be consistent).\n",
    "\n",
    "print(\"In order to find association, we need to calculate the Covariance.\")\n",
    "display(science_data[[\"RDExpen\", \"STJpapers\"]].cov())\n",
    "print(\n",
    "    \"The value of sxy\",\n",
    "    \"({:.2f})\".format(\n",
    "        science_data[[\"RDExpen\", \"STJpapers\"]].cov().at[\"RDExpen\", \"STJpapers\"]\n",
    "    ),\n",
    "    \"is different than zero, so we can say there is some degree of linear association between the variables.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Now we calculate the correlation strength by means of Pearson's linear correlation coefficient.\"\n",
    ")\n",
    "display(science_data[[\"RDExpen\", \"STJpapers\"]].corr() ** 2)\n",
    "print(\n",
    "    \"We can say there is a weak correlation\",\n",
    "    \"({:.2f})\".format(\n",
    "        (science_data[[\"RDExpen\", \"STJpapers\"]].corr() ** 2).at[\"RDExpen\", \"STJpapers\"]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8J7xYh_mZDvW"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"To talk about relationship, we draw a scatterplot and try to identify a function to describe de association.\"\n",
    ")\n",
    "sns.regplot(x=\"RDExpen\", y=\"STJpapers\", data=science_data)\n",
    "plt.xlabel(\"RDExpen\")\n",
    "plt.ylabel(\"STJpapers\")\n",
    "plt.title(\"Scatter Plot\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking with benevolent eyes, we can see a linear association (we already expected it to be weak)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tR6kFzDE9sAE"
   },
   "source": [
    "### 2.4.- <font color = \"Blue\"> Categoricals (2 Points)</font>\n",
    "\n",
    "**Using the split of question 1.5, make a graph that shows the relative frequencies of each category. Discuss it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6yibR__wxv6"
   },
   "outputs": [],
   "source": [
    "# we first create a contingency table\n",
    "cont_table = pd.crosstab(science_data.RDExpen_cat, science_data.STJpapers_cat).reindex([\"low\",\"mid\",\"high\"]).reindex([\"low\",\"mid\",\"high\"],axis=1)\n",
    "\n",
    "print(\"Contingency table:\")\n",
    "display(cont_table)\n",
    "print(\"\")\n",
    "\n",
    "# and use it to generate a bar plot\n",
    "print(\"Bar plot:\")\n",
    "freq = cont_table\n",
    "freq.plot(kind = 'bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that countries that expend lower amounts in R&D obtain low or mid number of papers, never high.<br>\n",
    "Countries that expend mid amounts in R&D get mid or high number of papers.<br>\n",
    "And finally, countries that expend higher amounts in R&D get also mid or high amounts of papers.\n",
    "\n",
    "One could conclude that it pays increasing the R&D budget and get into the mid cluster, the number of papers will be much higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8hJLwDo3_2Y"
   },
   "source": [
    "## 3.- <font color = \"Red\"> Probability </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnBwjcUd4GG3"
   },
   "source": [
    "### 3.1.- <font color = \"Blue\"> Depedency (2 Points) </font> \n",
    "\n",
    "**For this question you need to split the `RDExpen` and `STJpapers` in three levels denoted by `low`, `mid` and `high` (use the own range of the variables). Then, are the events of `high` level in both variables dependent or independent? Why?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqwLzzJI8Wa7"
   },
   "source": [
    "If both variables are dependent, the probability of the joint event \\begin{equation}\n",
    "P(STJpapers_{high}\\cap RDExpen_{high})\n",
    "\\end{equation} is different from 0.\n",
    "\n",
    "In order to get that information, we can reuse the contingency table calculated in 2.4 above, but normalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    science_data.RDExpen_cat, science_data.STJpapers_cat, normalize=True\n",
    ").reindex([\"low\", \"mid\", \"high\"]).reindex([\"low\", \"mid\", \"high\"], axis=1)[\"high\"][\n",
    "    \"high\"\n",
    "] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the value obtained is not 0, we can conclude both variables are dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY9xBABa4x4I"
   },
   "source": [
    "### 3.2.- <font color = \"Blue\"> Conditional Probability (2 Points) </font>\n",
    "\n",
    "**Find the probability that for any randomly chosen country, if the `RDExpen` is not high, the `STJpapers` is high**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the required probability we use Bayes' formula:\n",
    "\\begin{equation}\n",
    "P(STJpapers_{high}|RDExpens_{not\\_high})=\\frac{P(STJpapers_{high}\\cap RDExpens_{not\\_high})}{P(RDExpens_{not\\_high})}\n",
    "\\end{equation}\n",
    "\n",
    "And we can get both numerator and denominator from the bidimensional distribution (normalized contigency table if we simplify both axis to high and not high):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create two new categorical variables with high or not high values, using the limits calculated before\n",
    "science_data[\"RDExpen_cat2\"] = np.where(\n",
    "    np.isnan(science_data.RDExpen),\n",
    "    None,\n",
    "    (\n",
    "        np.where(\n",
    "            science_data.RDExpen <= science_data.RDExpen.quantile(0.75),\n",
    "            \"not_high\",\n",
    "            \"high\",\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "science_data[\"STJpapers_cat2\"] = np.where(\n",
    "    science_data.STJpapers <= science_data.STJpapers.quantile(0.75), \"not_high\", \"high\"\n",
    ")\n",
    "\n",
    "# and use these two new categorical variables to compute the bidimensional distribution\n",
    "bid_dist = (\n",
    "    pd.crosstab(science_data.RDExpen_cat2, science_data.STJpapers_cat2, normalize=True)\n",
    "    .reindex([\"not_high\", \"high\"])\n",
    "    .reindex([\"not_high\", \"high\"], axis=1)\n",
    ")\n",
    "display(bid_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we obtain the two values we are looking for. First, the probability of STJpaper being high if RDExpen is not_high\n",
    "P_STJpapersh_and_RDExpenNH = bid_dist[\"not_high\"][\"high\"]\n",
    "# and then the probability of RDExpen being not_high\n",
    "P_RDExpenNH = bid_dist[\"not_high\"].sum()\n",
    "\n",
    "# we compute it and obtain the conditional probability we were looking for\n",
    "P_STJpapersH_cond_RDExpenNH = P_STJpapersh_and_RDExpenNH / P_RDExpenNH\n",
    "print(\n",
    "    \"The conditional probability of STJpapers being high if RDExpen is not_high is\",\n",
    "    \"{:.2%}\".format(P_STJpapersH_cond_RDExpenNH),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slGnNGng5odR"
   },
   "source": [
    "### 3.3.- <font color = \"Blue\"> Expected Value (2 Points) </font> \n",
    "\n",
    "**Assuming that in the world, the research and development expenditure follows a normal distribution with mean 1.5 and standard deviation of 1.1, then find the expected number of countries in the sample that should have an expenditure above 3. Is that number in agreement with what you find in the data? Can you explain?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DbZ_vD65n4z"
   },
   "outputs": [],
   "source": [
    "# to better understand the problem, let's create a plot of the distribution\n",
    "limit = 3\n",
    "mu = 1.5\n",
    "s = 1.1\n",
    "minX = mu - 3.5 * s\n",
    "maxX = mu + 3.5 * s\n",
    "\n",
    "# we represent our normal distribution with 195 points, as there are 195 countries\n",
    "x = np.linspace(minX, maxX, 195)\n",
    "px1 = x[x > limit]\n",
    "y = norm.pdf(x=x, loc=mu, scale=s)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax1 = plt.subplot()\n",
    "ax1.plot(x, y)\n",
    "ax1.fill_between(x=px1, y1=0, y2=norm.pdf(px1, mu, s))\n",
    "ax1.vlines(x=limit, ymin=0, ymax=norm.pdf(limit, mu, s), linestyle=\"dotted\")\n",
    "# in order to obtain the probability of the rightmost region,\n",
    "# we compute the leftmost and substract it from 1\n",
    "ax1.set_title(\n",
    "    \"p(x >\" + str(limit) + \") =\" + str(np.round(1 - norm.cdf(limit, mu, s), 3))\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"We have obtained a probability of {np.round(1 - norm.cdf(limit, mu, s), 3)}. Applying it to our sample of\",\n",
    "    science_data.shape[0],\n",
    "    \"countries we obtain \\nthat\",\n",
    "    \"{:.0f}\".format(science_data.shape[0] * np.round(1 - norm.cdf(limit, mu, s), 3)),\n",
    "    \"countries in our sample should be above 3 in RDExpen\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_data.loc[science_data.RDExpen >= 3, \"RDExpen\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YEAH!!! :-)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exam I.ipynb",
   "provenance": [
    {
     "file_id": "1PZw1E53hgf2hXKpSXk5ohxedhzibo3mz",
     "timestamp": 1613076450222
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
